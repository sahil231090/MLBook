{"cells":[{"metadata":{"_cell_guid":"83708667-4fdc-1563-7b3a-06b6575d2865"},"cell_type":"markdown","source":"\n\n# Credit Card Fraud Detection\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\nAdditional Learning:\n* "},{"metadata":{},"cell_type":"markdown","source":"## Content"},{"metadata":{},"cell_type":"markdown","source":"* [1. Introduction](#0)\n* [2. Getting Started - Load Libraries and Dataset](#1)\n    * [2.1. Load Libraries](#1.1)    \n    * [2.2. Load Dataset](#1.2)\n* [3. Exploratory Data Analysis](#2)\n    * [3.1 Descriptive Statistics](#2.1)    \n    * [3.2. Data Visualisation](#2.2)\n* [4. Data Preparation](#3)\n    * [4.1 Data Cleaning](#3.1)    \n    * [4.2.Feature Selection](#3.2)\n    * [4.3.Data Transformation](#3.3) \n        * [4.3.1 Rescaling ](#3.3.1)\n        * [4.3.2 Standardization](#3.3.2)\n        * [4.3.3 Normalization](#3.3.3)    \n* [5.Evaluate Algorithms and Models](#4)        \n    * [5.1. Train/Test Split](#4.1)\n    * [5.2. Test Options and Evaluation Metrics](#4.2)\n    * [5.3. Compare Models and Algorithms](#4.3)\n        * [5.3.1 Common Classification Models](#4.3.1)\n        * [5.3.2 Ensemble Models](#4.3.2)\n        * [5.3.3 Deep Learning Models](#4.3.3)    \n* [6. Model Tuning and Grid Search](#5)  \n* [7. Finalize the Model](#6)  \n    * [7.1. Results on test dataset](#6.1)\n    * [7.1. Variable Intuition/Feature Selection](#6.2) \n    * [7.3. Save model for later use](#6.3)\n"},{"metadata":{},"cell_type":"markdown","source":"<a id='0'></a>\n# 1. Problem Definition"},{"metadata":{},"cell_type":"markdown","source":"The datasets contains transactions made by credit cards in September 2013 by european cardholders. \n\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. \n\nThe dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. \nFeature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n1. The features are mostly PC components(28 out of 30) which we can neither name nor engineer with. \n2. Since the dataset is highly imbalanced, using accuracy as evaluation metric is not a clever idea, the performance metrics should be Confusion Matrix and PR(precision-recall) curve, or maybe Area under PR curve if a single-number metric is needed.\n3. We should not predict hard labels(0 or 1) at first place but probabilities instead, since the default 0.5 cutoff for labeling might not be suitable for imbalanced dataset.\n4. As a brain refresher, Precision evaluates how many selected items are relevant, while Recall evaluates how many relevant items are selected.\n5. The train set is used to train model and model tuning, the validation set is used to evaluate on PR curve and choosing threshold, while the test set is only used for final prediction. \n"},{"metadata":{},"cell_type":"markdown","source":"<a id='1'></a>\n# 2. Getting Started- Loading the data and python packages"},{"metadata":{},"cell_type":"markdown","source":"<a id='1.1'></a>\n## 2.1. Loading the python packages"},{"metadata":{"_cell_guid":"5d8fee34-f454-2642-8b06-ed719f0317e1","trusted":true},"cell_type":"code","source":"# Load libraries\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot\nfrom pandas import read_csv, set_option\nfrom pandas.plotting import scatter_matrix\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n#Libraries for Deep Learning Models\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import SGD\n\n#Libraries for Saving the Model\nfrom pickle import dump\nfrom pickle import load","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='1.2'></a>\n## 2.2. Loading the Data"},{"metadata":{"_cell_guid":"787e35f7-bf9e-0969-8d13-a54fa87f3519","trusted":true},"cell_type":"code","source":"# load dataset\ndataset = pd.read_csv('../input/creditcard.csv')","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Diable the warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(dataset)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"df6a4523-b385-69ee-c933-592826d81431"},"cell_type":"markdown","source":"<a id='2'></a>\n# 3. Exploratory Data Analysis"},{"metadata":{},"cell_type":"markdown","source":"<a id='2.1'></a>\n## 3.1. Descriptive Statistics"},{"metadata":{"_cell_guid":"52f85dc2-0f91-3c50-400e-ddc38bea966b","trusted":true},"cell_type":"code","source":"# shape\ndataset.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# peek at data\nset_option('display.width', 100)\ndataset.head(5)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f36dd804-0c16-f0c9-05c9-d22b85a79e75","trusted":true},"cell_type":"code","source":"# types\nset_option('display.max_rows', 500)\ndataset.dtypes","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7bffeec0-5bbc-fffb-18f2-3da56b862ca3","trusted":true},"cell_type":"code","source":"# describe data\nset_option('precision', 3)\ndataset.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many are fraud and how many are not fraud ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"class_names = {0:'Not Fraud', 1:'Fraud'}\nprint(dataset.Class.value_counts().rename(index = class_names))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='2.2'></a>\n## 3.2. Data Visualization"},{"metadata":{"_cell_guid":"16d50177-f93e-9d26-af7a-313d7ebe9fcf","trusted":true},"cell_type":"code","source":"# histograms\ndataset.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1, figsize=(12,12))\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation\ncorrelation = dataset.corr()\npyplot.figure(figsize=(15,15))\npyplot.title('Correlation Matrix')\nsns.heatmap(correlation, vmax=1, square=True,annot=True,cmap='cubehelix')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Significant Correlation most of the areas are green or black."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scatterplot Matrix\n\n# from pandas.plotting import scatter_matrix\n# pyplot.figure(figsize=(15,15))\n# scatter_matrix(dataset,figsize=(12,12))\n# pyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='3'></a>\n## 4. Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"<a id='3.1'></a>\n## 4.1. Data Cleaning\nCheck for the NAs in the rows, either drop them or fill them with the mean of the column"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Checking for any null values and removing the null values'''\nprint('Null Values =',dataset.isnull().values.any())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Given that there are null values drop the rown contianing the null values."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Drop the rows containing NA\ndataset.dropna(axis=0)\n# Fill na with 0\n#dataset.fillna('0')\n\n#Filling the NAs with the mean of the column.\n#dataset['col'] = dataset['col'].fillna(dataset['col'].mean())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='3.2'></a>\n## 4.2. Handling Categorical Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"#There is no categorical Data, so it doen't need to be handled. ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='3.2'></a>\n## 4.2. Feature Selection\nStatistical tests can be used to select those features that have the strongest relationship with the output variable.The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features.\nThe example below uses the chi-squared (chi²) statistical test for non-negative features to select 10 of the best features from the Dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nbestfeatures = SelectKBest(score_func=chi2, k=10)\nbestfeatures","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y= dataset[\"Class\"]\nX = dataset.loc[:, dataset.columns != 'Class']\nfit = bestfeatures.fit(X,Y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be seem from the numbers above Credit Amount is the most important feature followed by duration."},{"metadata":{},"cell_type":"markdown","source":"<a id='3.3'></a>\n## 4.3. Data Transformation"},{"metadata":{},"cell_type":"markdown","source":"<a id='3.3.1'></a>\n### 4.3.1. Rescale Data\nWhen your data is comprised of attributes with varying scales, many machine learning algorithms\ncan benefit from rescaling the attributes to all have the same scale. Often this is referred to\nas normalization and attributes are often rescaled into the range between 0 and 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nX = dataset.loc[:, dataset.columns != 'Risk_Code']\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX = pd.DataFrame(scaler.fit_transform(X))\n# summarize transformed data\nrescaledX.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='3.3.2'></a>\n### 4.3.2. Standardize Data\nStandardization is a useful technique to transform attributes with a Gaussian distribution and\ndiffering means and standard deviations to a standard Gaussian distribution with a mean of\n0 and a standard deviation of 1."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nX = dataset.loc[:, dataset.columns != 'Risk_Code']\nscaler = StandardScaler().fit(X)\nStandardisedX = pd.DataFrame(scaler.fit_transform(X))\n# summarize transformed data\nStandardisedX.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='3.3.3'></a>\n### 4.3.1. Normalize Data\nNormalizing in scikit-learn refers to rescaling each observation (row) to have a length of 1 (called\na unit norm or a vector with the length of 1 in linear algebra)."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import Normalizer\nX = dataset.loc[:, dataset.columns != 'Risk_Code']\nscaler = Normalizer().fit(X)\nNormalizedX = pd.DataFrame(scaler.fit_transform(X))\n# summarize transformed data\nNormalizedX.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4'></a>\n# 5. Evaluate Algorithms and Models"},{"metadata":{},"cell_type":"markdown","source":"<a id='4.1'></a>\n## 5.1. Train Test Split"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split out validation dataset for the end\nY= dataset[\"Class\"]\nX = dataset.loc[:, dataset.columns != 'Class']\nscaler = StandardScaler().fit(X)\nStandardisedX = pd.DataFrame(scaler.fit_transform(X))\nvalidation_size = 0.2\nseed = 7\nX_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)","execution_count":12,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4.2'></a>\n## 5.2. Test Options and Evaluation Metrics\n"},{"metadata":{"_cell_guid":"5702bc31-06bf-8b6a-42de-366a6b3311a8","trusted":true},"cell_type":"code","source":"from sklearn.metrics import make_scorer, matthews_corrcoef\nMCC_scorer = make_scorer(matthews_corrcoef)\n# test options for classification\nnum_folds = 10\nseed = 7\n#scoring = 'accuracy'\n#scoring ='neg_log_loss'\n#scoring = 'roc_auc'","execution_count":17,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4.3'></a>\n## 5.3. Compare Models and Algorithms"},{"metadata":{},"cell_type":"markdown","source":"<a id='4.3.1'></a>\n### 5.3.1. Common Models"},{"metadata":{"_cell_guid":"772802f7-f4e4-84ee-6377-6464ab2e5da4","trusted":true},"cell_type":"code","source":"# spot check the algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\n#models.append(('SVM', SVC()))\n#Neural Network\n#models.append(('NN', MLPClassifier()))","execution_count":18,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4.3.2'></a>\n### 5.3.2. Ensemble Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Ensable Models \n# Boosting methods\nmodels.append(('AB', AdaBoostClassifier()))\nmodels.append(('GBM', GradientBoostingClassifier()))\n# Bagging methods\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('ET', ExtraTreesClassifier()))","execution_count":19,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='4.3.3'></a>\n### 5.3.3. Deep Learning Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Writing the Deep Learning Classifier in case the Deep Learning Flag is Set to True\n#Set the following Flag to 1 if the Deep LEarning Models Flag has to be enabled\nEnableDLModelsFlag = 0\nif EnableDLModelsFlag == 1 :   \n    # Function to create model, required for KerasClassifier\n    def create_model(neurons=12, activation='relu', learn_rate = 0.01, momentum=0):\n        # create model\n        model = Sequential()\n        model.add(Dense(neurons, input_dim=X_train.shape[1], activation=activation))\n        model.add(Dense(2, activation=activation))\n        model.add(Dense(1, activation='sigmoid'))\n        # Compile model\n        optimizer = SGD(lr=learn_rate, momentum=momentum)\n        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n        return model    \n    models.append(('DNN', KerasClassifier(build_fn=create_model, epochs=50, batch_size=10, verbose=0)))","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### K-folds cross validation"},{"metadata":{"_cell_guid":"a784ab4a-eb59-98cc-76cf-b55f382d057a","trusted":true},"cell_type":"code","source":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=MCC_scorer)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":21,"outputs":[{"output_type":"stream","text":"LR: 0.659537 (0.076228)\nLDA: 0.804188 (0.032573)\nKNN: 0.117666 (0.087253)\nCART: 0.740873 (0.062420)\nNB: 0.307124 (0.046851)\nAB: 0.741813 (0.066097)\nGBM: 0.636533 (0.170506)\nRF: 0.844617 (0.032692)\nET: 0.852621 (0.036221)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Algorithm comparison"},{"metadata":{"_cell_guid":"67873e9d-bc9b-6963-f594-805f1efbfbb3","trusted":true},"cell_type":"code","source":"# compare algorithms\nfig = pyplot.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(15,8)\npyplot.show()","execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1080x576 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"<a id='H5'></a>\n# Postprocessing the data : Data Standardisation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# standardized the dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()), ('LR', LogisticRegression())])))\npipelines.append(('ScaledLDA', Pipeline([('Scaler', StandardScaler()), ('LDA', LinearDiscriminantAnalysis())])))\npipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()), ('KNN', KNeighborsClassifier())])))\npipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()), ('CART', DecisionTreeClassifier())])))\npipelines.append(('ScaledNB', Pipeline([('Scaler', StandardScaler()), ('NB', GaussianNB())])))\n#pipelines.append(('ScaledSVM', Pipeline([('Scaler', StandardScaler()), ('SVM', SVC())])))\npipelines.append(('ScaledAB', Pipeline([('Scaler', StandardScaler()), ('CART', AdaBoostClassifier())])))\npipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()), ('GBM', GradientBoostingClassifier())])))\npipelines.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()), ('RF', RandomForestClassifier())])))\npipelines.append(('ScaledET', Pipeline([('Scaler', StandardScaler()), ('ET', ExtraTreesClassifier())])))\n#pipelines.append(('ScaledNN', Pipeline([('Scaler', StandardScaler()), ('NN', MLPClassifier())])))\n#pipelines.append(('ScaledDNN', Pipeline([('Scaler', StandardScaler()), ('DNN', KerasClassifier(build_fn=create_model, epochs=50, batch_size=10, verbose=0))])))","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"results = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=MCC_scorer)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","execution_count":26,"outputs":[{"output_type":"stream","text":"ScaledLR: 0.730228 (0.062326)\nScaledLDA: 0.804188 (0.032573)\nScaledKNN: 0.829572 (0.047373)\nScaledCART: 0.752681 (0.062105)\nScaledNB: 0.216258 (0.020926)\nScaledAB: 0.741813 (0.066097)\nScaledGBM: 0.639039 (0.172582)\nScaledRF: 0.829095 (0.055493)\nScaledET: 0.843456 (0.033981)\n","name":"stdout"}]},{"metadata":{"trusted":false},"cell_type":"code","source":"# compare scaled algorithms\nfig = pyplot.figure()\nfig.suptitle('Scaled Algorithm Comparison')\nax = fig.add_subplot(111)\npyplot.boxplot(results)\nax.set_xticklabels(names)\nfig.set_size_inches(12,8)\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='5'></a>\n# 6. Model Tuning and Grid Search"},{"metadata":{"_cell_guid":"848ca488-b0fd-8e93-2e68-23d32c71d89c"},"cell_type":"markdown","source":"Algorithm Tuning: Although some of the models show the most promising options. the grid search for Gradient Bossting Classifier is shown below."},{"metadata":{"trusted":false},"cell_type":"code","source":"# 1. Grid search : Logistic Regression Algorithm \n'''\npenalty : str, ‘l1’, ‘l2’, ‘elasticnet’ or ‘none’, optional (default=’l2’)\n\nC : float, optional (default=1.0)\nInverse of regularization strength; must be a positive float.Smaller values specify stronger regularization.\n''' \nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\ngrid={\"C\":np.logspace(-3,3,7), \"penalty\":[\"l1\",\"l2\"]}# l1 lasso l2 ridge\nC= np.logspace(-3,3,7)\npenalty = [\"l1\",\"l2\"]# l1 lasso l2 ridge\nparam_grid = dict(C=C,penalty=penalty )\nmodel = LogisticRegression()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\n#Print Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid Search : LDA Algorithm \n'''\nn_components : int, optional (default=None)\nNumber of components for dimensionality reduction. If None, will be set to min(n_classes - 1, n_features).\n''' \nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\ncomponents  = [1,3,5,7,9,11,13,15,17,19,600]\nparam_grid = dict(n_components=components)\nmodel = LinearDiscriminantAnalysis()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n#Print Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid Search KNN algorithm tuning\n'''\nn_neighbors : int, optional (default = 5)\n    Number of neighbors to use by default for kneighbors queries.\n\nweights : str or callable, optional (default = ‘uniform’)\n    weight function used in prediction. Possible values: ‘uniform’, ‘distance’\n\n''' \nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\n\nneighbors = [1,3,5,7,9,11,13,15,17,19,21]\nweights = ['uniform', 'distance']\nparam_grid = dict(n_neighbors=neighbors, weights = weights )\nmodel = KNeighborsClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\n#Print Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid Search : CART Algorithm \n'''\nmax_depth : int or None, optional (default=None)\n    The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure \n    or until all leaves contain less than min_samples_split samples.\n\n''' \nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmax_depth = np.arange(2, 30)\nparam_grid = dict(max_depth=max_depth)\nmodel = DecisionTreeClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n#Print Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid Search : NB algorithm tuning\n#GaussianNB only accepts priors as an argument so unless you have some priors to set for your model ahead of time \n#you will have nothing to grid search over.\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid Search: SVM algorithm tuning\n'''\nC : float, optional (default=1.0)\nPenalty parameter C of the error term.\n\nkernel : string, optional (default=’rbf’)\nSpecifies the kernel type to be used in the algorithm. \nIt must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or a callable. \nParameters of SVM are C and kernel. \nTry a number of kernels with various values of C with less bias and more bias (less than and greater than 1.0 respectively\n''' \nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nc_values = [0.1, 0.3, 0.5, 0.7, 0.9, 1.0, 1.3, 1.5]\nkernel_values = ['linear', 'poly', 'rbf']\nparam_grid = dict(C=c_values, kernel=kernel_values)\nmodel = SVC()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\n#Print Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid Search: Ada boost Algorithm Tuning \n'''\nn_estimators : integer, optional (default=50)\n    The maximum number of estimators at which boosting is terminated. \n    In case of perfect fit, the learning procedure is stopped early.\n''' \nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nn_estimators = [10, 100]\nparam_grid = dict(n_estimators=n_estimators)\nmodel = AdaBoostClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\n#Print Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid Search: GradientBoosting Tuning\n'''\nn_estimators : int (default=100)\n    The number of boosting stages to perform. \n    Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\nmax_depth : integer, optional (default=3)\n    maximum depth of the individual regression estimators. \n    The maximum depth limits the number of nodes in the tree. \n    Tune this parameter for best performance; the best value depends on the interaction of the input variables.\n\n''' \nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nn_estimators = [20,180]\nmax_depth= [3,5]\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth)\nmodel = GradientBoostingClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\n#Print Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid Search: Random Forest Classifier\n'''\nn_estimators : int (default=100)\n    The number of boosting stages to perform. \n    Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\nmax_depth : integer, optional (default=3)\n    maximum depth of the individual regression estimators. \n    The maximum depth limits the number of nodes in the tree. \n    Tune this parameter for best performance; the best value depends on the interaction of the input variables    \ncriterion : string, optional (default=”gini”)\n    The function to measure the quality of a split. \n    Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. \n    \n'''   \nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nn_estimators = [20,80]\nmax_depth= [5,10]\ncriterion = [\"gini\",\"entropy\"]\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth, criterion = criterion )\nmodel = RandomForestClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\n#Print Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid Search: ExtraTreesClassifier()\n'''\nn_estimators : int (default=100)\n    The number of boosting stages to perform. \n    Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\nmax_depth : integer, optional (default=3)\n    maximum depth of the individual regression estimators. \n    The maximum depth limits the number of nodes in the tree. \n    Tune this parameter for best performance; the best value depends on the interaction of the input variables    \ncriterion : string, optional (default=”gini”)\n    The function to measure the quality of a split. \n    Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. \n'''   \nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nn_estimators = [20,80]\nmax_depth= [5,10]\ncriterion = [\"gini\",\"entropy\"]\nparam_grid = dict(n_estimators=n_estimators, max_depth=max_depth, criterion = criterion )\nmodel = ExtraTreesClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\n#Print Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid Search : NN algorithm tuning\n'''\nhidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n    The ith element represents the number of neurons in the ith hidden layer.\nOther Parameters that can be tuned\n    learning_rate_init : double, optional, default 0.001\n        The initial learning rate used. It controls the step-size in updating the weights. Only used when solver=’sgd’ or ‘adam’.\n    max_iter : int, optional, default 200\n        Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.\n''' \nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nhidden_layer_sizes=[(20,), (50,), (20,20), (20, 30, 20)]\nparam_grid = dict(hidden_layer_sizes=hidden_layer_sizes)\nmodel = MLPClassifier()\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\n#Print Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Grid Search : Deep Neural Network algorithm tuning\n'''\nneurons: int\n    Number of patterns shown to the network before the weights are updated.     \nbatch_size: int\n    Number of observation to read at a time and keep in memory.\nepochs: int\n    Number of times that the entire training dataset is shown to the network during training.\nactivation:\n    The activation function controls the non-linearity of individual neurons and when to fire.\nlearn_rate :int\n    controls how much to update the weight at the end of each batch\nmomentum : int\n     momentum controls how much to let the previous update influence the current weight update\n''' \nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\n#Hyperparameters that can be modified\nneurons = [1, 5, 10, 15]\nbatch_size = [10, 20, 40, 60, 80, 100]\nepochs = [10, 50, 100]\nactivation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\nlearn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\nmomentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n\n#Changing only Neurons for the sake of simplicity\nparam_grid = dict(neurons=neurons)\nmodel = KerasClassifier(build_fn=create_model, epochs=50, batch_size=10, verbose=0)\nkfold = KFold(n_splits=num_folds, random_state=seed)\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\ngrid_result = grid.fit(rescaledX, Y_train)\n\n#Print Results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nranks = grid_result.cv_results_['rank_test_score']\nfor mean, stdev, param, rank in zip(means, stds, params, ranks):\n    print(\"#%d %f (%f) with: %r\" % (rank, mean, stdev, param))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6'></a>\n# 7. Finalise the Model"},{"metadata":{},"cell_type":"markdown","source":"Looking at the details above GBM might be worthy of further study, but for now SVM shows a lot of promise as a low complexity and stable model for this problem.\n\nFinalize Model with best parameters found during tuning step."},{"metadata":{},"cell_type":"markdown","source":"<a id='6.1'></a>\n## 7.1. Results on the Test Dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"# prepare model\nscaler = StandardScaler().fit(X_train)\nrescaledX = scaler.transform(X_train)\nmodel = GradientBoostingClassifier(n_estimators=20, max_depth=5) # rbf is default kernel\nmodel.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"f9725666-3c21-69d1-ddf6-45e47d982444","trusted":false},"cell_type":"code","source":"# estimate accuracy on validation set\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(X_validation)\nprint(accuracy_score(Y_validation, predictions))\nprint(confusion_matrix(Y_validation, predictions))\nprint(classification_report(Y_validation, predictions))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"Y_validation","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6.2'></a>\n## 7.2. Variable Intuition/Feature Importance\nLooking at the details above GBM might be worthy of further study, but for now SVM shows a lot of promise as a low complexity and stable model for this problem.\nLet us look into the Feature Importance of the GBM model"},{"metadata":{"trusted":false},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nmodel = GradientBoostingClassifier()\nmodel.fit(rescaledX,Y_train)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\npyplot.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id='6.3'></a>\n## 7.3. Save Model for Later Use"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Save Model Using Pickle\nfrom pickle import dump\nfrom pickle import load\n\n# save the model to disk\nfilename = 'finalized_model.sav'\ndump(model, open(filename, 'wb'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# some time later...\n# load the model from disk\nloaded_model = load(open(filename, 'rb'))\n# estimate accuracy on validation set\nrescaledValidationX = scaler.transform(X_validation)\npredictions = model.predict(rescaledValidationX)\nresult = accuracy_score(Y_validation, predictions)\nprint(result)","execution_count":null,"outputs":[]}],"metadata":{"_change_revision":206,"_is_fork":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}